### 任务：扩充核心词汇库至考研级别

#### 1. 当前项目状态总结

经过近期的集中开发，我们的“句读 (SentenCease)”项目已经从一个基础原型，演进为一个功能完备、体验优良的全栈应用。目前的具体开发进度如下：

-   **全栈技术架构稳固**: 我们成功搭建并验证了整个技术栈，包括 Go 语言后端、React 前端、PostgreSQL 数据库，并实现了完整的 Docker 容器化部署。
-   **核心功能闭环**: 用户认证（注册、登录）、基于间隔重复系统（SRS）的动态词汇学习（获取下一个词、提交复习进度）等核心功能已经全部实现并可以正常工作。
-   **UI/UX 彻底革新**: 我们对前端应用进行了一次彻底的 UI/UX 大修，所有页面和组件都应用了以 Tailwind CSS 为核心的现代化、移动优先的设计，显著提升了用户体验。
-   **关键Bug修复**: 成功定位并根除了一个导致学习进度无法更新的后端Bug。通过统一数据模型中的ID类型，我们确保了数据在整个系统中的一致性和正确性，保证了应用的核心功能稳定可靠。

**当前成果**: 项目已达到“体验良好且核心功能稳定”的阶段，为下一步的内容扩充和功能增强打下了坚实的基础。

#### 2. 下一步核心目标：大规模扩充词汇库

**核心需求**: 为了让“句读”真正成为一个实用的备考工具，我们需要将其当前的词汇库，从示例级别扩充至一本完整的 **考研英语大纲词汇** 的规模。

**目标**:
-   覆盖约5500个考研核心词汇。
-   为每个（或绝大部分）核心词汇提供高质量、语境丰富的例句和精准的释义。
-   处理好“一词多义”的情况，将不同词义作为独立的学习项。

#### 3. 技术方案与执行步骤

这个任务的核心是数据工程，我们需要一个可靠、可重复的流程来将外部词汇数据导入到我们的PostgreSQL数据库中。

**第一阶段：数据采集与结构化**

1.  **寻找数据源**: 寻找一个开源、权威的考研词汇列表。这可以是一个结构化的文件（如JSON, CSV），或是一个公开的API。
2.  **定义数据格式**: 我们需要将采集到的数据整理成统一的JSON格式。每个单词对象应包含以下结构：
    ```json
    {
      "lemma": "abandon",
      "meanings": [
        {
          "part_of_speech": "verb",
          "definition": "to leave a place, thing, or person, usually for ever",
          "example_sentence": "We had to abandon the car.",
          "example_sentence_translation": "我们不得不弃车而去。"
        },
        {
          "part_of_speech": "verb",
          "definition": "to stop doing an activity before you have finished it",
          "example_sentence": "The match was abandoned because of bad weather.",
          "example_sentence_translation": "因为天气恶劣，比赛被取消了。"
        }
      ]
    }
    ```

**第二阶段：开发数据库填充脚本**

1.  **创建Go脚本**: 在 `backend/cmd` 目录下创建一个新的Go程序（例如 `seeder/main.go`）。这个程序不作为服务运行，而是一个一次性的命令行工具。
2.  **解析数据**: 脚本需要能够读取并解析我们在第一阶段准备好的JSON数据文件。
3.  **写入数据库**:
    *   遍历JSON中的每一个单词。
    *   首先，查询 `words` 表，如果单词的 `lemma` 不存在，则插入新单词。
    *   然后，遍历该单词下的所有 `meanings`。
    *   对于每一个 `meaning`，查询 `meanings` 表，检查 `(word_id, definition)` 组合是否已存在，以避免重复插入。
    *   如果不存在，则将新的词义、例句等信息插入到 `meanings` 表中。
4.  **实现幂等性**: 整个脚本必须是“幂等”的，即无论运行多少次，数据库的最终状态都应该是一样的，不会因为重复运行而产生垃圾数据。

**第三阶段：集成与执行**

1.  **更新`docker-compose.yml`**: 我们可以添加一个新的服务或修改现有服务，以便在需要时运行这个填充脚本。一个简单的策略是在后端服务启动前，通过一个一次性任务来执行数据填充。
2.  **执行填充**: 运行填充脚本，将数千条词汇数据一次性、可靠地导入生产（或开发）数据库。

#### 4. 你的任务

你的任务是领导并执行以上三个阶段的开发工作。这需要你具备数据处理、Go编程和数据库操作的能力。请从 **第一阶段：数据采集与结构化** 开始。如果你能找到合适的考研词汇数据源，请告诉我，我们可以一起评估它的质量。 